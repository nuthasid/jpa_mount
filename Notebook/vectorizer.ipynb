{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "    \n",
    "from main_fit_vectorizer import TFIDF_Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vectorizer_1234.pck', 'rb') as pf:\n",
    "    vectorizer = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do tokenize\n",
    "## return tokenized list\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, n_gram, stop_en=None, stop_th=None, keyword=None):\n",
    "\n",
    "        import re\n",
    "        import deepcut\n",
    "        import os\n",
    "        from nltk.tokenize import TreebankWordTokenizer\n",
    "        from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "        self.test_text = 'ตัวอย่างความต้องการใช้ตัวอย่างความต้องการลีนุ๊กซ์การใช้ยากลำบาก'\n",
    "        self.pattern_thai_char = re.compile(u'[\\u0e00-\\u0e7f]')\n",
    "        self.pattern_new_sentence = re.compile('\\.[0-9]+(\\)|\\.) ')\n",
    "        self.pattern_th_out = re.compile(u'[\\u0e00-\\u0e7f][^\\u0e00-\\u0e7f]')\n",
    "        self.pattern_th_in = re.compile(u'[^\\u0e00-\\u0e7f][\\u0e00-\\u0e7f]')\n",
    "        self.pattern_num_bullet = re.compile('^[0-9]+(\\)|\\.)*$')\n",
    "        self.pattern_end_token = re.compile('^[a-zA-Z]+$')\n",
    "        self.pattern_number = re.compile('\\+*[0-9]+')\n",
    "        self.pattern_phone_number = re.compile('[0-9]+-[0-9]+-[0-9]+')\n",
    "        self.pattern_email = re.compile('[a-zA-Z._\\-0-9]+@[a-zA-Z._\\-0-9]+')\n",
    "        self.pattern_url = re.compile('(https://|www.)[a-zA-Z0-9]+.[a-z]+[^\\s]*')\n",
    "        self.pattern_sentence_collide = re.compile('[a-z][A-Z]]')\n",
    "        self.pattern_thai_name = re.compile(u'\\u0e04\\u0e38\\u0e13\\s*[\\u0e00-\\u0e7f]+\\s+')\n",
    "        self.charset = {}\n",
    "        with open(os.path.join(os.getcwd(), '..', 'Resource', 'charset'), 'rt') as charfile:\n",
    "            for item in charfile.read().split('\\n'):\n",
    "                if len(item) < 4:\n",
    "                    self.charset[item] = ord(item)\n",
    "                else:\n",
    "                    self.charset[chr(int(item, 16))] = int(item, 16)\n",
    "        self.eng_tokenizer = TreebankWordTokenizer()\n",
    "        self.stemming = EnglishStemmer()\n",
    "        self.n_gram = n_gram\n",
    "        self.dp = deepcut\n",
    "        if stop_en:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'Resource', stop_en), 'rt', encoding='utf-8') as stop_file:\n",
    "                self.stop_en = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.stop_en = set([])\n",
    "        if stop_th:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'Resource', stop_th), 'rt', encoding='utf-8') as stop_file:\n",
    "                self.stop_th = set([item for item in stop_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.stop_th = set([])\n",
    "        if keyword:\n",
    "            with open(os.path.join(os.getcwd(), '..', 'Resource', keyword), 'rt', encoding='utf-8') as keyword_file:\n",
    "                self.keyword = set([item for item in keyword_file.read().split('\\n')])\n",
    "        else:\n",
    "            self.keyword = set([])\n",
    "            \n",
    "    def tokenizer(self, text=None):\n",
    "\n",
    "        def n_gram_compile(tokens, n):\n",
    "\n",
    "            tokens = tokens[:]\n",
    "            n_tokens = []\n",
    "            if n <= 1:\n",
    "                return tokens\n",
    "            for j, token in enumerate(tokens[:-(n - 1)]):\n",
    "                new_token = ''\n",
    "                for word in tokens[j:j + n]:\n",
    "                    if self.pattern_thai_char.search(word) and len(word) > 1:\n",
    "                        new_token += word\n",
    "                    else:\n",
    "                        new_token = ''\n",
    "                        break\n",
    "                if new_token:\n",
    "                    n_tokens.extend([new_token])\n",
    "            return n_tokens\n",
    "\n",
    "        def n_grams_compile(tokens, n):\n",
    "\n",
    "            if n < 2:\n",
    "                return tokens\n",
    "            n_tokens = []\n",
    "            for j in range(2, n + 1):\n",
    "                n_tokens.extend(n_gram_compile(tokens, j))\n",
    "            n_tokens = tokens + n_tokens\n",
    "            return n_tokens\n",
    "\n",
    "        def validate_char(val_text):\n",
    "            val_text = val_text.replace('&amp;', ' ')\n",
    "            val_text = val_text.replace('&nbsp;', ' ')\n",
    "            ret_text = ''\n",
    "            for cha in val_text:\n",
    "                try:\n",
    "                    self.charset[cha]\n",
    "                except KeyError:\n",
    "                    ret_text += ' '\n",
    "                else:\n",
    "                    ret_text += cha\n",
    "            while ret_text.find('  ') != -1:\n",
    "                ret_text = ret_text.replace('  ', ' ')\n",
    "            return ret_text\n",
    "        \n",
    "        def split_th_en(splt_text):\n",
    "            insert_pos = []\n",
    "            splt_text = splt_text[:]\n",
    "            for pos, item in enumerate(splt_text[:-2]):\n",
    "                if self.pattern_th_in.search(splt_text[pos:pos+2]) or self.pattern_th_out.search(splt_text[pos:pos+2]):\n",
    "                    insert_pos.append(pos + 1)\n",
    "            for pos in reversed(insert_pos):\n",
    "                splt_text = splt_text[:pos] + ' ' + splt_text[pos:]\n",
    "            return splt_text\n",
    "\n",
    "        def remove_thai_stop(th_text):\n",
    "            stop_pos = [[0, 0]]\n",
    "            ## TH : do longest matching\n",
    "            for j in range(len(th_text)-1):\n",
    "                for k in range(j+1, len(th_text)):\n",
    "                    if th_text[j:k] in self.stop_th:\n",
    "                        # found keyword +++ instead of returning string - return positions that is\n",
    "                        # i to j\n",
    "                        if j <= stop_pos[-1][1]:\n",
    "                            stop_pos[-1] = [stop_pos[-1][0], k]\n",
    "                        else:\n",
    "                            stop_pos.append([j, k])\n",
    "                        break\n",
    "            newstr = ''\n",
    "            if len(stop_pos) == 1:\n",
    "                newstr = th_text\n",
    "            else:\n",
    "                for j in range(len(stop_pos)-1):\n",
    "                    newstr += th_text[stop_pos[j][1]:stop_pos[j+1][0]] + ' '\n",
    "            return newstr\n",
    "\n",
    "        if text == '-test':\n",
    "            text = self.test_text\n",
    "            \n",
    "        text = text.replace(u'\\u0e46', ' ')\n",
    "        text = self.pattern_email.sub(' ', text)\n",
    "        text = self.pattern_url.sub(' ', text)\n",
    "        text = self.pattern_phone_number.sub(' ', text)\n",
    "        text = self.pattern_thai_name.sub(' ', text)\n",
    "        text = split_th_en(text)\n",
    "        text = self.pattern_new_sentence.sub(' . ', text)\n",
    "        text = text.replace('.', ' . ')\n",
    "        text = validate_char(text)\n",
    "        text = remove_thai_stop(text)\n",
    "        text_split = text.split(' ')\n",
    "        text_split = [item for item in text_split[:] if item not in self.stop_en\n",
    "                      and not self.pattern_num_bullet.search(item)]\n",
    "        text_split = [self.stemming.stem(item) if self.pattern_end_token.search(item) and\n",
    "                      item not in self.keyword else item for item in text_split[:]]\n",
    "\n",
    "        first_pass = []\n",
    "        for i, item in enumerate(text_split):\n",
    "            if self.pattern_sentence_collide.search(item) and item not in self.keyword:\n",
    "                c_text = self.pattern_sentence_collide.search(item)\n",
    "                first_pass.extend([c_text.string[:c_text.span()[0]+1], c_text.string[c_text.span()[1]-1:]])\n",
    "            else:\n",
    "                first_pass.append(item)\n",
    "        second_pass = []\n",
    "        for i, chunk in enumerate(first_pass):\n",
    "            if self.pattern_thai_char.search(chunk) and len(chunk) > 1:\n",
    "                new_chunk = self.dp.tokenize(chunk)\n",
    "                second_pass.extend(new_chunk)\n",
    "            else:\n",
    "                second_pass.append(chunk.lower())\n",
    "\n",
    "        second_pass = n_grams_compile(second_pass, self.n_gram)\n",
    "\n",
    "        return set(second_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "tkn1 = Tokenizer(1)\n",
    "tkn2 = Tokenizer(2)\n",
    "## tkn3 = Tokenizer(3, 'en_stop_word.txt', 'th_stop_word.txt')\n",
    "tkn4 = Tokenizer(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class DataController():\n",
    "    dataMatrix = pd.DataFrame(columns=[\"title\",\"desc\",\"tag\"])\n",
    "    \n",
    "    ## init will create dataMatrix\n",
    "    def __init__(self, pathToFile):\n",
    "        import os\n",
    "        import json\n",
    "        count = 0\n",
    "        \n",
    "        with open(pathToFile, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                ## for each line, add into dataMatrix, using [\"title\", \"desc\", \"tag\"] structure\n",
    "                line_dict = json.loads(line, encoding='utf-8')\n",
    "                self.dataMatrix = self.dataMatrix.append(line_dict, ignore_index=True)\n",
    "                #count+=1\n",
    "                #if(count==100): break\n",
    "    \n",
    "    def getTrainingSet(self, label_class):\n",
    "        ## classSet is set of data that has tag = label_class\n",
    "        targetSet = self.dataMatrix[self.dataMatrix['tag']==label_class]\n",
    "        restSet = self.dataMatrix[self.dataMatrix['tag']!=label_class]\n",
    "\n",
    "        if(targetSet.shape[0] < restSet.shape[0]):\n",
    "            # target has less population than the rest\n",
    "            trainingSet = pd.concat([targetSet, restSet.sample(n=targetSet.shape[0])])\n",
    "        else:\n",
    "            # target has more population than the rest\n",
    "            trainingSet = pd.concat([targetSet.sample(n=restSet.shape[0]), restSet])\n",
    "        # shuffle data using sample fraction = 1\n",
    "        trainingSet = trainingSet.sample(frac=1)\n",
    "        return trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "file_name = \"block1234.json\"\n",
    "file_path = os.getcwd()+\"/../data/\"+file_name\n",
    "\n",
    "data = DataController(file_path)\n",
    "\n",
    "## Create training data\n",
    "trainingData = data.getTrainingSet(\"0\")\n",
    "\n",
    "training_Desc = trainingData['desc'] \n",
    "training_Title = trainingData['title']\n",
    "training_Label = trainingData['tag']\n",
    "\n",
    "## vectorize data\n",
    "\n",
    "# desc_vectorizer = vectorizer.vectorize_desc\n",
    "desc_vectorizer = TfidfVectorizer(tokenizer=tkn2.tokenizer, max_df=1.0, min_df=1)\n",
    "desc_vec = desc_vectorizer.fit_transform(training_Title)\n",
    "\n",
    "# title_vectorizer = vectorizer.vectorize_title\n",
    "title_vectorizer = TfidfVectorizer(tokenizer=tkn4.tokenizer, max_df=1.0, min_df=1)\n",
    "title_vec = title_vectorizer.fit_transform(training_Desc)\n",
    "\n",
    "## stack title onto desc\n",
    "from scipy.sparse import hstack\n",
    "data_vec = hstack([title_vec, desc_vec])\n",
    "\n",
    "## create label_vec\n",
    "label_vec = training_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train using Multinomial NaiveBayes \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## see crossvalidation score\n",
    "NBclf = MultinomialNB()\n",
    "scores = cross_val_score(NBclf, data_vec, label_vec, cv=3, scoring='f1_macro')\n",
    "print('Cross validation score: ', scores)\n",
    "\n",
    "## split sample into train_set and test_set\n",
    "desc_train, desc_test, label_train, label_test = train_test_split(data_vec, label_vec, test_size=0.3)\n",
    "\n",
    "## In sample accuracy\n",
    "in_NBclf = MultinomialNB()\n",
    "in_NBclf = in_NBclf.fit(data_vec, label_vec)\n",
    "label_predict = in_NBclf.predict(data_vec)\n",
    "print(classification_report(label_vec, label_predict))\n",
    "\n",
    "## Test set accuracy\n",
    "NBclf = NBclf.fit(desc_train, label_train)\n",
    "label_predict = NBclf.predict(desc_test)\n",
    "print(classification_report(label_test, label_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
